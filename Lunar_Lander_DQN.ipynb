{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgGZUR4aP7zz"
      },
      "source": [
        "# Lunar Lander OpenAI gym environment\n",
        "\n",
        "Check out the [Open AI Gym documentation](https://www.gymlibrary.dev/environments/box2d/lunar_lander/) for a full description of the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7qrtoK1PqFs"
      },
      "source": [
        "## Action Space\n",
        "\n",
        "The agent has four discrete actions available:\n",
        "\n",
        "* Do nothing = 0\n",
        "* Fire right engine = 1\n",
        "* Fire main engine = 2\n",
        "* Fire left engine = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PDfsCYVQCjp"
      },
      "source": [
        "## Observation Space\n",
        "\n",
        "The agent's observation space consists of a state vector with 8 variables:\n",
        "\n",
        "* Its $(x,y)$ coordinates. The landing pad is always at coordinates $(0,0)$.\n",
        "* Its linear velocities $(\\dot x,\\dot y)$.\n",
        "* Its angle $\\theta$.\n",
        "* Its angular velocity $\\dot \\theta$.\n",
        "* Two booleans, $l$ and $r$, that represent whether each leg is in contact with the ground or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2K-PqdnQmP3"
      },
      "source": [
        "## Rewards\n",
        "\n",
        "After every step, a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "- is increased/decreased the closer/further the lander is to the landing pad.\n",
        "- is increased/decreased the slower/faster the lander is moving.\n",
        "- is decreased the more the lander is tilted (angle not horizontal).\n",
        "- is increased by 10 points for each leg that is in contact with the ground.\n",
        "- is decreased by 0.03 points each frame a side engine is firing.\n",
        "- is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcK_Wy6gQorL"
      },
      "source": [
        "## Episode Termination\n",
        "\n",
        "An episode ends (i.e the environment enters a terminal state) if:\n",
        "\n",
        "* The lunar lander crashes (i.e if the body of the lunar lander comes in contact with the surface of the moon).\n",
        "\n",
        "* The absolute value of the lander's $x$-coordinate is greater than 1 (i.e. it goes beyond the left or right border)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sk7XR2WQ_b5"
      },
      "source": [
        "# env setup\n",
        "\n",
        "Finally got the gym environment to run on colab with the help of this [post](https://stackoverflow.com/questions/76222239/pip-install-gymnasiumbox2d-not-working-on-google-colab) after searching everywhere for some solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ii1GXK_eUC9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4799a8-440e-4c32-b1d1-08752fdfa7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.6/1.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install swig --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jR49bNhdUJXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6062fd-879c-4c3c-827f-361f8051ae52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[box2d] --quiet --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mJ22bxo-ljQL"
      },
      "outputs": [],
      "source": [
        "# import gymnasium as gym\n",
        "import gym\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ae2l2Xi6UyeJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29aa33c8-2fce-42c9-d012-3375340a5524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# # Ignore all warnings\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7KxpovXNTmD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902d349d-1662-4bea-c394-80454c6dbe86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (8,)\n",
            "Number of actions:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\n",
        "    \"LunarLander-v2\",\n",
        "    continuous = False,\n",
        "    gravity = -10.0,\n",
        "    enable_wind = False,\n",
        "    wind_power = 15.0,\n",
        "    turbulence_power = 1.5,\n",
        ")\n",
        "env.reset(seed=0)\n",
        "\n",
        "state_shape = env.observation_space.shape\n",
        "num_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)\n",
        "print('Number of actions: ', num_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "61XEg7UnVgW7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "5758c32e-c60c-407e-a70c-6f0db226f177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "Image.fromarray(env.render(mode='rgb_array'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find some way to render the game\n",
        "\n",
        "This took way too long than I would have expected. This all has to do with finding a work-around for the rendering of the environment, because env.render() caused me too many issues to count, as I wanted to use google colab.\n",
        "\n",
        "Finally stuck with [this solution](https://stackoverflow.com/questions/60765613/how-to-show-episode-in-rendered-openai-gym-environment) from from stackoverflow.\n"
      ],
      "metadata": {
        "id": "tnC2AnAbIQ36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import imageio\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import PIL.ImageDraw as ImageDraw\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "from moviepy.editor import VideoFileClip\n",
        "nb_dir = os.path.abspath('')"
      ],
      "metadata": {
        "id": "fc6FilB1iNXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4adfb4-f36d-46a5-f557-263d3712443b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/skimage/util/dtype.py:27: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  np.bool8: (False, True),\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please use `sobel` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _label_with_episode_number(frame, episode_num):\n",
        "  im = Image.fromarray(frame)\n",
        "  drawer = ImageDraw.Draw(im)\n",
        "  if np.mean(im) < 128:\n",
        "    text_color = (255,255,255)\n",
        "  else:\n",
        "    text_color = (0,0,0)\n",
        "  drawer.text((im.size[0]/20,im.size[1]/18), f'Episode: {episode_num+1}', fill=text_color)\n",
        "  return im\n",
        "\n",
        "def save_gif(frames_list, gif_file_name, frames_per_second=30):\n",
        "  imageio.mimwrite(gif_file_name, frames, fps=frames_per_second)\n",
        "\n",
        "def gif_to_mp4(gif_file_name, mp4_file_name, playback_speed):\n",
        "  # Convert the saved gif to a .mp4 video format\n",
        "  gif_path = os.path.join(nb_dir, gif_file_name)\n",
        "  clip = VideoFileClip(gif_path)\n",
        "  adjusted_clip = clip.speedx(factor=playback_speed)\n",
        "  mp4_path = os.path.join(nb_dir, mp4_file_name)\n",
        "  adjusted_clip.write_videofile(mp4_path)\n",
        "\n",
        "def download_gameplay_video(frames_list, mp4_file_name, frames_per_second=30, playback_speed=1):\n",
        "  gif_file_name = mp4_file_name[:-4] + '.gif'\n",
        "  save_gif(frames_list, gif_file_name, frames_per_second)\n",
        "  gif_to_mp4(gif_file_name, mp4_file_name, playback_speed)\n",
        "  # Download the video file into your local system to view it\n",
        "  from google.colab import files\n",
        "  mp4_path = os.path.join(nb_dir, mp4_file_name)\n",
        "  files.download(mp4_path)"
      ],
      "metadata": {
        "id": "z6YKY_QIiQcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1bf89e0-02b4-4b22-83f5-c852da9d74a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample usage\n",
        "\n",
        "Make sure that your code follows a similar structure to the cell below when you want to record the gameplay and view it later.\n",
        "\n",
        "Don't run the cell below if you don't want to download the gameplay of the random agent. It's only meant to be for reference on how to use the functions in the cell above."
      ],
      "metadata": {
        "id": "l_Y8c69NjCu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames = []\n",
        "num_episodes = 2 # number of times to play the game from the beginning, keep this low else it will eat up all the RAM of colab\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()\n",
        "  while True:\n",
        "    # Make sure to do write these 2 lines of code in the appropriate position when you are training the model\n",
        "    # (along with the frames = [] at the very beginning of the cell) :\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    frames.append(_label_with_episode_number(frame, episode_num=episode))\n",
        "\n",
        "    random_action = env.action_space.sample()\n",
        "    state, _, done, _ = env.step(random_action)\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "file_name = 'Random Agent.mp4'\n",
        "download_gameplay_video(frames_list=frames, mp4_file_name=file_name, frames_per_second=30, playback_speed=1)"
      ],
      "metadata": {
        "id": "VgZx_G9WiMRM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "30747d31-df81-4032-f815-01989174f6ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/imageio/plugins/pillow.py:390: DeprecationWarning: The keyword `fps` is no longer supported. Use `duration`(in ms) instead, e.g. `fps=50` == `duration=20` (1000 * 1/50).\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/Random Agent.mp4.\n",
            "Moviepy - Writing video /content/Random Agent.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/Random Agent.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_39269b54-0c05-48af-a02e-40295c6e2798\", \"Random Agent.mp4\", 43056)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCB-pK2iYy0Z"
      },
      "source": [
        "# Deep Q-Learning (DQN)\n",
        "\n",
        "In cases where both the state and action space are discrete we can estimate the action-value function iteratively by using the Bellman equation:\n",
        "\n",
        "$$\n",
        "Q_{i+1}(s,a) = R + \\gamma \\max_{a'}Q_i(s',a')\n",
        "$$\n",
        "\n",
        "This iterative method converges to the optimal action-value function $Q^*(s,a)$ as $i\\to\\infty$. This means that the agent just needs to gradually explore the state-action space and keep updating the estimate of $Q(s,a)$ until it converges to the optimal action-value function $Q^*(s,a)$. However, in cases where the state space is continuous it becomes practically impossible to explore the entire state-action space. Consequently, this also makes it practically impossible to gradually estimate $Q(s,a)$ until it converges to $Q^*(s,a)$.\n",
        "\n",
        "In the Deep $Q$-Learning, we solve this problem by using a neural network to estimate the action-value function $Q(s,a)\\approx Q^*(s,a)$. We call this neural network a $Q$-Network and it can be trained by adjusting its weights at each iteration to minimize the mean-squared error in the Bellman equation.\n",
        "\n",
        "Unfortunately, using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable. Luckily, there's a couple of techniques that can be employed to avoid instabilities. These techniques consist of using a ***Target Network*** and ***Experience Replay***.\n",
        "\n",
        "The purpose served by these 2 techniques, is also the explanation behind why just a single neural network fails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsTssGgeZxVl"
      },
      "source": [
        "## Target Netork\n",
        "\n",
        "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
        "\n",
        "$$\n",
        "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
        "$$\n",
        "\n",
        "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
        "\n",
        "$$\n",
        "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
        "$$\n",
        "\n",
        "Notice that this forms a problem because the $y$ target is changing on every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create\n",
        "a separate neural network for generating the $y$ targets. We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
        "\n",
        "$$\n",
        "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
        "$$\n",
        "\n",
        "where $w^-$ and $w$ are the weights of the target $\\hat Q$-Network and $Q$-Network, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cNG7g7najH_"
      },
      "source": [
        "## Soft Updates\n",
        "\n",
        "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
        "\n",
        "$$\n",
        "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
        "$$\n",
        "\n",
        "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MAqUPXsXVIQ"
      },
      "source": [
        "## Experience Replay\n",
        "\n",
        "When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this, we employ a technique known as Experience Replay to generate uncorrelated experiences for training our agent.\n",
        "\n",
        "Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
        "\n",
        "By using experience replay we avoid problematic correlations, oscillations and instabilities. In addition, experience replay also allows the agent to potentially use the same experience in multiple weight updates, which increases data efficiency.\n",
        "\n",
        "For convenience, we will store the experiences as named tuples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8dexlDUavgw"
      },
      "source": [
        "## Putting everything together\n",
        "\n",
        "Now that we know all the techniques that we are going to use, we can put them together to arrive at the Deep Q-Learning Algorithm with a Target Network and an Experience Replay.\n",
        "\n",
        "[DQN Algo](https://drive.google.com/file/d/1OhkSQAG6TU8-1svWzaZHfCn1LDN9FTA6/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZWM3ZKGhs_-"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO6BEkZCmK-h"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SBTfN6qqmMX0"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "# Fixed seed for reproducibility of results\n",
        "rs = 1234\n",
        "torch.manual_seed(rs)\n",
        "random.seed(rs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# .to(device) --> on all tensors and models to speed up calculations by utilizing gpu\n",
        "# if models are on gpu and tensors aren on cpu, then pytorch would give a RuntimeError about the same\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc2Ubj2b5uv9",
        "outputId": "47d5a6dd-f2e0-44f8-9a0d-ec33eb434e2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "FHGv3_dEdYtI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m2EY4PAvWSDM"
      },
      "outputs": [],
      "source": [
        "TARGET_UPDATE_FREQ = 4 # update the target Q network with the parameters of the Q network every this number of steps\n",
        "TAU = 0.001 # soft update parameter\n",
        "ALPHA = 0.001  # learning rate for the optimizer\n",
        "GAMMA = 0.995  # discount rate\n",
        "\n",
        "# Replay buffer parameters:\n",
        "BATCH_SIZE = 64 # number of experiences we will sample from the replay buffer, for training\n",
        "BUFFER_SIZE = 50_000 # maximum number of experiences we will store in the replay buffer before overwriting old experiences\n",
        "MIN_BUFFER_SIZE = 250 # how many experiences we want in the replay buffer before we want to start training by computing gradients\n",
        "\n",
        "# Epsilon greedy policy for exploration vs exploitation by the agent (nn model) in the environment:\n",
        "EPSILON_START = 1.0 # go from 100% random actions\n",
        "EPSILON_END = 0.01 # to 1% random actions\n",
        "EPSILON_DECAY_RATE = 0.02 # decay rate of Epsilon over the course of training\n",
        "\n",
        "# Number of training episodes and logging frequency:\n",
        "NUM_EPISODES = 2000 # number of games that we will run the training loop for\n",
        "LOG_STEPS = 1000 # log the results of training every this many steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the Network class"
      ],
      "metadata": {
        "id": "czMytFkjdRxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, env):\n",
        "    super().__init__()\n",
        "    in_features = int(np.prod(env.observation_space.shape))\n",
        "    self.net = nn.Sequential (\n",
        "      nn.Linear(in_features, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, env.action_space.n)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "  def act(self, state):\n",
        "    state_t = torch.as_tensor(state, dtype=torch.float32).to(device) # convert the state into a pytorch tensor data type\n",
        "    q_values = self.forward(state_t.unsqueeze(0)) # get the q values for every single action that the agent can take\n",
        "    # unsqueeze(0) for an extra dummy batch dimension = 1, as we are not using a batched environment\n",
        "    max_q_index = torch.argmax(q_values, dim=1)[0]\n",
        "    action = max_q_index.detach().item() # convert the pytorch tensor max_q_index into an integer which is the\n",
        "    # action index in the action space that gives the greatest q value (according to this nn) among all the other actions\n",
        "    return action"
      ],
      "metadata": {
        "id": "XJPOATXgdFXh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the Q and Target Q Networks and an optimizer"
      ],
      "metadata": {
        "id": "XGgDR2tHdNeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_net = Network(env).to(device)\n",
        "target_q_net = Network(env).to(device)\n",
        "\n",
        "# Initialize the weights of both the Q and the target Q networks with the same weights\n",
        "target_q_net.load_state_dict(q_net.state_dict())\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.Adam(q_net.parameters(), lr=ALPHA)"
      ],
      "metadata": {
        "id": "JmR8k2FldGk1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure optimizer's state corresponds to the model's device\n",
        "for state in optimizer.state.values():\n",
        "  for k, v in state.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "      state[k] = v.to(device)"
      ],
      "metadata": {
        "id": "GJiMlLAgFOEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca902b9-56e6-4bdf-c0f1-d379a763c490"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a replay buffer and a reward buffer"
      ],
      "metadata": {
        "id": "tEhU5EaCdgM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
        "# Initialize replay buffer\n",
        "state = env.reset()\n",
        "for _ in range(MIN_BUFFER_SIZE):\n",
        "  random_action = env.action_space.sample()\n",
        "  new_state, reward, done, info = env.step(random_action)\n",
        "  experience = (state, random_action, reward, done, new_state)\n",
        "  replay_buffer.append(experience)\n",
        "  state = new_state\n",
        "  if done:\n",
        "    state = env.reset()\n",
        "\n",
        "# Make a reward buffer for storing the total cumulative reward in each episode upto the last 100 episodes\n",
        "reward_buffer = deque([0.0], maxlen=100)"
      ],
      "metadata": {
        "id": "3B2c7r5DdVmx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a function to get the required values from the replay buffer as tuples"
      ],
      "metadata": {
        "id": "7DJv80ecdqai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_from_replay_buffer():\n",
        "  experiences = random.sample(replay_buffer, BATCH_SIZE)\n",
        "  # print(experiences[0])\n",
        "  states_list = [e[0].tolist() for e in experiences]\n",
        "  states_array = np.array(states_list)\n",
        "  states_t = torch.as_tensor(states_array, dtype=torch.float32).to(device)\n",
        "  # print(states_t[0])\n",
        "  actions_list = [e[1] for e in experiences]\n",
        "  actions_array = np.array(actions_list)\n",
        "  actions_t = torch.as_tensor(actions_array, dtype=torch.int64).unsqueeze(-1).to(device)\n",
        "  # print(actions_t[0])\n",
        "  rewards_list = [e[2] for e in experiences]\n",
        "  rewards_array = np.array(rewards_list)\n",
        "  rewards_t = torch.as_tensor(rewards_array, dtype=torch.float32).unsqueeze(-1).to(device)\n",
        "  # print(rewards_t[0])\n",
        "  dones_list = [e[3] for e in experiences]\n",
        "  dones_array = np.array(dones_list)\n",
        "  dones_t = torch.as_tensor(dones_array, dtype=torch.float32).unsqueeze(-1).to(device)\n",
        "  # print(dones_t[0])\n",
        "  new_states_list = [e[4].tolist() for e in experiences]\n",
        "  new_states_array = np.array(new_states_list)\n",
        "  new_states_t = torch.as_tensor(new_states_array, dtype=torch.float32).to(device)\n",
        "  # print(new_states_t[0])\n",
        "  return states_t, actions_t, rewards_t, dones_t, new_states_t"
      ],
      "metadata": {
        "id": "ri6yquTKlfSv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epsilon Greedy Exploration with decay over time (episodes)"
      ],
      "metadata": {
        "id": "OPUFs-x7Ubo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_epsilon(episode):\n",
        "  epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-(EPSILON_DECAY_RATE * episode)) # argument was episode and rate was 0.015\n",
        "  # epsilon = EPSILON_DECAY_RATE * epsilon # argument was epsilon and rate was 0.995\n",
        "  return max(epsilon, EPSILON_END)"
      ],
      "metadata": {
        "id": "zPSIRAfWUk70"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the agent"
      ],
      "metadata": {
        "id": "E_H9PzfllmMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_DQN_Agent(Q_net_params_filepath):\n",
        "  # flags for storing the parameters only once in every checkpoint\n",
        "  saved_checkpoint_1 = True\n",
        "  saved_checkpoint_2 = True\n",
        "  saved_checkpoint_3 = True\n",
        "  saved_checkpoint_4 = True\n",
        "\n",
        "  # For plotting the results later\n",
        "  episode_avg_reward_dict = {}\n",
        "\n",
        "  step = 0\n",
        "\n",
        "  # epsilon = EPSILON_START\n",
        "  for episode in tqdm(range(NUM_EPISODES), desc='Training...', leave=True):\n",
        "    episode_reward = 0.0\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "      epsilon = decay_epsilon(episode)\n",
        "      random_sample = random.random()\n",
        "      action = None\n",
        "      if random_sample <= epsilon:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = q_net.act(state)\n",
        "\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      experience = (state, action, reward, done, new_state)\n",
        "      replay_buffer.append(experience)\n",
        "      state = new_state\n",
        "\n",
        "      episode_reward += reward\n",
        "\n",
        "      if done:\n",
        "        reward_buffer.append(episode_reward)\n",
        "        break\n",
        "\n",
        "      # Sample a batch of experiences from the replay_buffer to train the model\n",
        "      states, actions, rewards, dones, new_states = get_sample_from_replay_buffer()\n",
        "\n",
        "      # Compute targets\n",
        "      target_q_values = target_q_net(new_states) # set of q values for each observation in the batch\n",
        "      max_target_values = target_q_values.max(dim=1, keepdim=True)[0] # choose the max q value for each observation\n",
        "      targets = rewards + GAMMA * (1 - dones) * max_target_values # if step is a terminal step then 2nd term would be 0\n",
        "      # this is a short way to write the piece-wise definition of the targets\n",
        "\n",
        "      # Compute loss\n",
        "      q_values = q_net(states)\n",
        "      action_q_values = torch.gather(input=q_values, dim=1, index=actions)\n",
        "      # loss = nn.MSELoss(action_q_values, targets)\n",
        "      loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "\n",
        "      # Gradient Descent\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Update target network\n",
        "      if step % TARGET_UPDATE_FREQ == 0:\n",
        "        for target_param, param in zip(target_q_net.parameters(), q_net.parameters()):\n",
        "          target_param.data.copy_(TAU * param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "      # Logging results and saving checkpoints in training\n",
        "      if step % LOG_STEPS == 0 and step != 0:\n",
        "        print('Step No:', step)\n",
        "        print('Episode No:', episode)\n",
        "        avg_reward = np.mean(reward_buffer)\n",
        "        print('Avg reward over the last 100 episodes:', avg_reward)\n",
        "        episode_avg_reward_dict[episode] = avg_reward\n",
        "        print('Highest avg reward logged so far:', max(episode_avg_reward_dict.values()) )\n",
        "        # Save the weights of the model once it reaches certain points so that we can play\n",
        "        # the game using different models and see its gameplay in specific stages of its training\n",
        "        if avg_reward  > 50 and saved_checkpoint_1:\n",
        "          q_net_params_filepath = Q_net_params_filepath[:-4] + ' Checkpoint 1' + '.pth'\n",
        "          torch.save(target_q_net.state_dict(), q_net_params_filepath)\n",
        "          print(f\"Agent parameters saved successfully to '{q_net_params_filepath}'\")\n",
        "          saved_checkpoint_1 = False\n",
        "        if avg_reward > 150 and saved_checkpoint_2:\n",
        "          q_net_params_filepath = Q_net_params_filepath[:-4] + ' Checkpoint 2' + '.pth'\n",
        "          torch.save(target_q_net.state_dict(), q_net_params_filepath)\n",
        "          print(f\"Agent parameters saved successfully to '{q_net_params_filepath}'\")\n",
        "          saved_checkpoint_2 = False\n",
        "        if avg_reward >= 200 and saved_checkpoint_3:\n",
        "          q_net_params_filepath = Q_net_params_filepath[:-4] + ' Checkpoint 3' + '.pth'\n",
        "          torch.save(target_q_net.state_dict(), q_net_params_filepath)\n",
        "          print(f\"Agent parameters saved successfully to '{q_net_params_filepath}'\")\n",
        "          saved_checkpoint_3 = False\n",
        "        if avg_reward >= 230 and saved_checkpoint_4:\n",
        "          q_net_params_filepath = Q_net_params_filepath[:-4] + ' Checkpoint 4' + '.pth'\n",
        "          torch.save(target_q_net.state_dict(), q_net_params_filepath)\n",
        "          print(f\"Agent parameters saved successfully to '{q_net_params_filepath}'\")\n",
        "          saved_checkpoint_4 = False\n",
        "          print()\n",
        "          print(f\"Environment solved successfully in {episode+1} episodes !\")\n",
        "          return episode_avg_reward_dict\n",
        "        print()\n",
        "\n",
        "      step += 1\n",
        "\n",
        "  return episode_avg_reward_dict"
      ],
      "metadata": {
        "id": "M5lcF2v2lm-e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_net_params_filepath = os.path.join(nb_dir, 'Q net params.pth')\n",
        "\n",
        "episode_avg_reward_dict = train_DQN_Agent(Q_net_params_filepath=Q_net_params_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c450a0c1f18e42f1a42e85ad966eea98",
            "e53c80b3f60c4408b5b0b3762b03a186",
            "b0e471d558e242c49dc24939a52fb9c8",
            "af3361d8f7f9441ebea67cfcb9bb211f",
            "309a6f9539fd4b738a9fb1f649d55158",
            "127ba5b0bad24db5a1c456fde70f9ee0",
            "0e5b9fb30c794269820e8a518bca0dad",
            "799d065ddb814e75acb79d542aeb78c5",
            "2f2d409c1f0f40d6967633e890981334",
            "1aa31c8ba96044c2a3e788e956b2ff5e",
            "28f656d85a884cbfabadb9dc1b0c03d3"
          ]
        },
        "id": "jNSpvkOzEGEk",
        "outputId": "3afb3c0a-69dd-4d38-fdf2-742381b01047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training...:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c450a0c1f18e42f1a42e85ad966eea98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step No: 1000\n",
            "Episode No: 11\n",
            "Avg reward over the last 100 episodes: -110.31482118447451\n",
            "Highest avg reward logged so far: -110.31482118447451\n",
            "\n",
            "Step No: 2000\n",
            "Episode No: 19\n",
            "Avg reward over the last 100 episodes: -111.25365991719426\n",
            "Highest avg reward logged so far: -110.31482118447451\n",
            "\n",
            "Step No: 3000\n",
            "Episode No: 20\n",
            "Avg reward over the last 100 episodes: -113.19589460208512\n",
            "Highest avg reward logged so far: -110.31482118447451\n",
            "\n",
            "Step No: 4000\n",
            "Episode No: 27\n",
            "Avg reward over the last 100 episodes: -115.2194800888333\n",
            "Highest avg reward logged so far: -110.31482118447451\n",
            "\n",
            "Step No: 5000\n",
            "Episode No: 29\n",
            "Avg reward over the last 100 episodes: -107.0795588854228\n",
            "Highest avg reward logged so far: -107.0795588854228\n",
            "\n",
            "Step No: 6000\n",
            "Episode No: 34\n",
            "Avg reward over the last 100 episodes: -104.25734664173208\n",
            "Highest avg reward logged so far: -104.25734664173208\n",
            "\n",
            "Step No: 7000\n",
            "Episode No: 39\n",
            "Avg reward over the last 100 episodes: -106.27673564891693\n",
            "Highest avg reward logged so far: -104.25734664173208\n",
            "\n",
            "Step No: 8000\n",
            "Episode No: 42\n",
            "Avg reward over the last 100 episodes: -106.46378688296174\n",
            "Highest avg reward logged so far: -104.25734664173208\n",
            "\n",
            "Step No: 9000\n",
            "Episode No: 43\n",
            "Avg reward over the last 100 episodes: -101.37661592145493\n",
            "Highest avg reward logged so far: -101.37661592145493\n",
            "\n",
            "Step No: 10000\n",
            "Episode No: 47\n",
            "Avg reward over the last 100 episodes: -97.64770149477106\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 11000\n",
            "Episode No: 51\n",
            "Avg reward over the last 100 episodes: -100.23784896993973\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 12000\n",
            "Episode No: 55\n",
            "Avg reward over the last 100 episodes: -101.61218305528415\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 13000\n",
            "Episode No: 59\n",
            "Avg reward over the last 100 episodes: -102.70652928373597\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 14000\n",
            "Episode No: 60\n",
            "Avg reward over the last 100 episodes: -104.13279312547758\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 15000\n",
            "Episode No: 62\n",
            "Avg reward over the last 100 episodes: -102.97939561109717\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 16000\n",
            "Episode No: 64\n",
            "Avg reward over the last 100 episodes: -103.67167049978806\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 17000\n",
            "Episode No: 66\n",
            "Avg reward over the last 100 episodes: -103.35727504205028\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 18000\n",
            "Episode No: 69\n",
            "Avg reward over the last 100 episodes: -101.61257211044685\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 19000\n",
            "Episode No: 72\n",
            "Avg reward over the last 100 episodes: -102.78159335276138\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 20000\n",
            "Episode No: 75\n",
            "Avg reward over the last 100 episodes: -99.7358454402753\n",
            "Highest avg reward logged so far: -97.64770149477106\n",
            "\n",
            "Step No: 21000\n",
            "Episode No: 78\n",
            "Avg reward over the last 100 episodes: -97.27594611103895\n",
            "Highest avg reward logged so far: -97.27594611103895\n",
            "\n",
            "Step No: 22000\n",
            "Episode No: 80\n",
            "Avg reward over the last 100 episodes: -98.18732318933901\n",
            "Highest avg reward logged so far: -97.27594611103895\n",
            "\n",
            "Step No: 23000\n",
            "Episode No: 82\n",
            "Avg reward over the last 100 episodes: -98.1002868213337\n",
            "Highest avg reward logged so far: -97.27594611103895\n",
            "\n",
            "Step No: 24000\n",
            "Episode No: 83\n",
            "Avg reward over the last 100 episodes: -97.46685868053389\n",
            "Highest avg reward logged so far: -97.27594611103895\n",
            "\n",
            "Step No: 25000\n",
            "Episode No: 85\n",
            "Avg reward over the last 100 episodes: -93.28881088588327\n",
            "Highest avg reward logged so far: -93.28881088588327\n",
            "\n",
            "Step No: 26000\n",
            "Episode No: 86\n",
            "Avg reward over the last 100 episodes: -94.00229160184398\n",
            "Highest avg reward logged so far: -93.28881088588327\n",
            "\n",
            "Step No: 27000\n",
            "Episode No: 89\n",
            "Avg reward over the last 100 episodes: -89.05056238099316\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 28000\n",
            "Episode No: 90\n",
            "Avg reward over the last 100 episodes: -89.80900135209764\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 29000\n",
            "Episode No: 92\n",
            "Avg reward over the last 100 episodes: -91.47095926485038\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 30000\n",
            "Episode No: 93\n",
            "Avg reward over the last 100 episodes: -91.77478558106927\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 31000\n",
            "Episode No: 94\n",
            "Avg reward over the last 100 episodes: -92.33973941748313\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 32000\n",
            "Episode No: 95\n",
            "Avg reward over the last 100 episodes: -92.02846700135024\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 33000\n",
            "Episode No: 96\n",
            "Avg reward over the last 100 episodes: -91.71981613965956\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 34000\n",
            "Episode No: 97\n",
            "Avg reward over the last 100 episodes: -92.28538512740893\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 35000\n",
            "Episode No: 98\n",
            "Avg reward over the last 100 episodes: -91.88466387756564\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 36000\n",
            "Episode No: 99\n",
            "Avg reward over the last 100 episodes: -92.46883388476412\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 37000\n",
            "Episode No: 101\n",
            "Avg reward over the last 100 episodes: -92.6722997580879\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 38000\n",
            "Episode No: 102\n",
            "Avg reward over the last 100 episodes: -93.4178124176834\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 39000\n",
            "Episode No: 103\n",
            "Avg reward over the last 100 episodes: -91.09454159377432\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 40000\n",
            "Episode No: 104\n",
            "Avg reward over the last 100 episodes: -91.2191445936864\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 41000\n",
            "Episode No: 106\n",
            "Avg reward over the last 100 episodes: -90.5622152363881\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 42000\n",
            "Episode No: 107\n",
            "Avg reward over the last 100 episodes: -90.77471769134031\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 43000\n",
            "Episode No: 108\n",
            "Avg reward over the last 100 episodes: -90.41261800341239\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 44000\n",
            "Episode No: 109\n",
            "Avg reward over the last 100 episodes: -89.16417940266945\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 45000\n",
            "Episode No: 110\n",
            "Avg reward over the last 100 episodes: -89.80722830537186\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 46000\n",
            "Episode No: 111\n",
            "Avg reward over the last 100 episodes: -90.20041747577974\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 47000\n",
            "Episode No: 112\n",
            "Avg reward over the last 100 episodes: -90.40694483688125\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 48000\n",
            "Episode No: 113\n",
            "Avg reward over the last 100 episodes: -90.0218439073892\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 49000\n",
            "Episode No: 114\n",
            "Avg reward over the last 100 episodes: -89.28639682548574\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 50000\n",
            "Episode No: 115\n",
            "Avg reward over the last 100 episodes: -89.91272844738779\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 51000\n",
            "Episode No: 116\n",
            "Avg reward over the last 100 episodes: -91.37786537216277\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 52000\n",
            "Episode No: 117\n",
            "Avg reward over the last 100 episodes: -90.27623429994594\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 53000\n",
            "Episode No: 118\n",
            "Avg reward over the last 100 episodes: -90.23110734286266\n",
            "Highest avg reward logged so far: -89.05056238099316\n",
            "\n",
            "Step No: 54000\n",
            "Episode No: 119\n",
            "Avg reward over the last 100 episodes: -88.67382166168551\n",
            "Highest avg reward logged so far: -88.67382166168551\n",
            "\n",
            "Step No: 55000\n",
            "Episode No: 120\n",
            "Avg reward over the last 100 episodes: -88.52865335956285\n",
            "Highest avg reward logged so far: -88.52865335956285\n",
            "\n",
            "Step No: 56000\n",
            "Episode No: 121\n",
            "Avg reward over the last 100 episodes: -90.66504177329783\n",
            "Highest avg reward logged so far: -88.52865335956285\n",
            "\n",
            "Step No: 57000\n",
            "Episode No: 122\n",
            "Avg reward over the last 100 episodes: -90.21879712789685\n",
            "Highest avg reward logged so far: -88.52865335956285\n",
            "\n",
            "Step No: 58000\n",
            "Episode No: 123\n",
            "Avg reward over the last 100 episodes: -87.70647284107062\n",
            "Highest avg reward logged so far: -87.70647284107062\n",
            "\n",
            "Step No: 59000\n",
            "Episode No: 124\n",
            "Avg reward over the last 100 episodes: -87.26960581559015\n",
            "Highest avg reward logged so far: -87.26960581559015\n",
            "\n",
            "Step No: 60000\n",
            "Episode No: 125\n",
            "Avg reward over the last 100 episodes: -87.01881089221408\n",
            "Highest avg reward logged so far: -87.01881089221408\n",
            "\n",
            "Step No: 61000\n",
            "Episode No: 126\n",
            "Avg reward over the last 100 episodes: -86.62434767130961\n",
            "Highest avg reward logged so far: -86.62434767130961\n",
            "\n",
            "Step No: 62000\n",
            "Episode No: 127\n",
            "Avg reward over the last 100 episodes: -85.694887844231\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 63000\n",
            "Episode No: 128\n",
            "Avg reward over the last 100 episodes: -87.79513279035798\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 64000\n",
            "Episode No: 129\n",
            "Avg reward over the last 100 episodes: -87.89128714930087\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 65000\n",
            "Episode No: 130\n",
            "Avg reward over the last 100 episodes: -86.51802741350858\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 66000\n",
            "Episode No: 131\n",
            "Avg reward over the last 100 episodes: -86.31629679725341\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 67000\n",
            "Episode No: 132\n",
            "Avg reward over the last 100 episodes: -87.41824545340619\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 68000\n",
            "Episode No: 133\n",
            "Avg reward over the last 100 episodes: -87.0063413008205\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 69000\n",
            "Episode No: 134\n",
            "Avg reward over the last 100 episodes: -86.64644099665072\n",
            "Highest avg reward logged so far: -85.694887844231\n",
            "\n",
            "Step No: 70000\n",
            "Episode No: 135\n",
            "Avg reward over the last 100 episodes: -85.69388440731812\n",
            "Highest avg reward logged so far: -85.69388440731812\n",
            "\n",
            "Step No: 71000\n",
            "Episode No: 136\n",
            "Avg reward over the last 100 episodes: -85.38220581539788\n",
            "Highest avg reward logged so far: -85.38220581539788\n",
            "\n",
            "Step No: 72000\n",
            "Episode No: 137\n",
            "Avg reward over the last 100 episodes: -84.64799066109171\n",
            "Highest avg reward logged so far: -84.64799066109171\n",
            "\n",
            "Step No: 73000\n",
            "Episode No: 138\n",
            "Avg reward over the last 100 episodes: -84.254459928486\n",
            "Highest avg reward logged so far: -84.254459928486\n",
            "\n",
            "Step No: 74000\n",
            "Episode No: 139\n",
            "Avg reward over the last 100 episodes: -82.68593826658348\n",
            "Highest avg reward logged so far: -82.68593826658348\n",
            "\n",
            "Step No: 75000\n",
            "Episode No: 140\n",
            "Avg reward over the last 100 episodes: -81.02637038790789\n",
            "Highest avg reward logged so far: -81.02637038790789\n",
            "\n",
            "Step No: 76000\n",
            "Episode No: 141\n",
            "Avg reward over the last 100 episodes: -80.50478736425958\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 77000\n",
            "Episode No: 142\n",
            "Avg reward over the last 100 episodes: -80.69287098141344\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 78000\n",
            "Episode No: 143\n",
            "Avg reward over the last 100 episodes: -82.55519469803002\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 79000\n",
            "Episode No: 144\n",
            "Avg reward over the last 100 episodes: -81.87700249447452\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 80000\n",
            "Episode No: 145\n",
            "Avg reward over the last 100 episodes: -82.27787478431095\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 81000\n",
            "Episode No: 146\n",
            "Avg reward over the last 100 episodes: -82.53991036232921\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 82000\n",
            "Episode No: 147\n",
            "Avg reward over the last 100 episodes: -82.078467133334\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 83000\n",
            "Episode No: 148\n",
            "Avg reward over the last 100 episodes: -81.74395605805331\n",
            "Highest avg reward logged so far: -80.50478736425958\n",
            "\n",
            "Step No: 84000\n",
            "Episode No: 149\n",
            "Avg reward over the last 100 episodes: -80.47078206643654\n",
            "Highest avg reward logged so far: -80.47078206643654\n",
            "\n",
            "Step No: 85000\n",
            "Episode No: 150\n",
            "Avg reward over the last 100 episodes: -80.10834933565799\n",
            "Highest avg reward logged so far: -80.10834933565799\n",
            "\n",
            "Step No: 86000\n",
            "Episode No: 151\n",
            "Avg reward over the last 100 episodes: -78.66331068668578\n",
            "Highest avg reward logged so far: -78.66331068668578\n",
            "\n",
            "Step No: 87000\n",
            "Episode No: 152\n",
            "Avg reward over the last 100 episodes: -77.58396178757363\n",
            "Highest avg reward logged so far: -77.58396178757363\n",
            "\n",
            "Step No: 88000\n",
            "Episode No: 153\n",
            "Avg reward over the last 100 episodes: -77.20050205009876\n",
            "Highest avg reward logged so far: -77.20050205009876\n",
            "\n",
            "Step No: 89000\n",
            "Episode No: 154\n",
            "Avg reward over the last 100 episodes: -77.09368848075295\n",
            "Highest avg reward logged so far: -77.09368848075295\n",
            "\n",
            "Step No: 90000\n",
            "Episode No: 155\n",
            "Avg reward over the last 100 episodes: -76.06689220298048\n",
            "Highest avg reward logged so far: -76.06689220298048\n",
            "\n",
            "Step No: 91000\n",
            "Episode No: 156\n",
            "Avg reward over the last 100 episodes: -74.77136575840883\n",
            "Highest avg reward logged so far: -74.77136575840883\n",
            "\n",
            "Step No: 92000\n",
            "Episode No: 157\n",
            "Avg reward over the last 100 episodes: -72.60971060489197\n",
            "Highest avg reward logged so far: -72.60971060489197\n",
            "\n",
            "Step No: 93000\n",
            "Episode No: 158\n",
            "Avg reward over the last 100 episodes: -72.17013765970866\n",
            "Highest avg reward logged so far: -72.17013765970866\n",
            "\n",
            "Step No: 94000\n",
            "Episode No: 159\n",
            "Avg reward over the last 100 episodes: -72.11695488091773\n",
            "Highest avg reward logged so far: -72.11695488091773\n",
            "\n",
            "Step No: 95000\n",
            "Episode No: 160\n",
            "Avg reward over the last 100 episodes: -71.15247096678307\n",
            "Highest avg reward logged so far: -71.15247096678307\n",
            "\n",
            "Step No: 96000\n",
            "Episode No: 161\n",
            "Avg reward over the last 100 episodes: -71.09860482819347\n",
            "Highest avg reward logged so far: -71.09860482819347\n",
            "\n",
            "Step No: 97000\n",
            "Episode No: 162\n",
            "Avg reward over the last 100 episodes: -71.14976260018302\n",
            "Highest avg reward logged so far: -71.09860482819347\n",
            "\n",
            "Step No: 98000\n",
            "Episode No: 163\n",
            "Avg reward over the last 100 episodes: -71.82022694450472\n",
            "Highest avg reward logged so far: -71.09860482819347\n",
            "\n",
            "Step No: 99000\n",
            "Episode No: 164\n",
            "Avg reward over the last 100 episodes: -70.07289093022192\n",
            "Highest avg reward logged so far: -70.07289093022192\n",
            "\n",
            "Step No: 100000\n",
            "Episode No: 165\n",
            "Avg reward over the last 100 episodes: -70.00139451454802\n",
            "Highest avg reward logged so far: -70.00139451454802\n",
            "\n",
            "Step No: 101000\n",
            "Episode No: 166\n",
            "Avg reward over the last 100 episodes: -68.95547075033055\n",
            "Highest avg reward logged so far: -68.95547075033055\n",
            "\n",
            "Step No: 102000\n",
            "Episode No: 167\n",
            "Avg reward over the last 100 episodes: -68.72526347503225\n",
            "Highest avg reward logged so far: -68.72526347503225\n",
            "\n",
            "Step No: 103000\n",
            "Episode No: 168\n",
            "Avg reward over the last 100 episodes: -68.78070055600642\n",
            "Highest avg reward logged so far: -68.72526347503225\n",
            "\n",
            "Step No: 104000\n",
            "Episode No: 170\n",
            "Avg reward over the last 100 episodes: -66.61877277448163\n",
            "Highest avg reward logged so far: -66.61877277448163\n",
            "\n",
            "Step No: 105000\n",
            "Episode No: 171\n",
            "Avg reward over the last 100 episodes: -66.36086104306625\n",
            "Highest avg reward logged so far: -66.36086104306625\n",
            "\n",
            "Step No: 106000\n",
            "Episode No: 172\n",
            "Avg reward over the last 100 episodes: -65.51961033428326\n",
            "Highest avg reward logged so far: -65.51961033428326\n",
            "\n",
            "Step No: 107000\n",
            "Episode No: 173\n",
            "Avg reward over the last 100 episodes: -65.76574929083714\n",
            "Highest avg reward logged so far: -65.51961033428326\n",
            "\n",
            "Step No: 108000\n",
            "Episode No: 174\n",
            "Avg reward over the last 100 episodes: -65.5426943896556\n",
            "Highest avg reward logged so far: -65.51961033428326\n",
            "\n",
            "Step No: 109000\n",
            "Episode No: 175\n",
            "Avg reward over the last 100 episodes: -65.72829750845075\n",
            "Highest avg reward logged so far: -65.51961033428326\n",
            "\n",
            "Step No: 110000\n",
            "Episode No: 176\n",
            "Avg reward over the last 100 episodes: -65.51365933881941\n",
            "Highest avg reward logged so far: -65.51365933881941\n",
            "\n",
            "Step No: 111000\n",
            "Episode No: 177\n",
            "Avg reward over the last 100 episodes: -65.69333491793172\n",
            "Highest avg reward logged so far: -65.51365933881941\n",
            "\n",
            "Step No: 112000\n",
            "Episode No: 181\n",
            "Avg reward over the last 100 episodes: -64.32342998745675\n",
            "Highest avg reward logged so far: -64.32342998745675\n",
            "\n",
            "Step No: 113000\n",
            "Episode No: 182\n",
            "Avg reward over the last 100 episodes: -63.80607589936644\n",
            "Highest avg reward logged so far: -63.80607589936644\n",
            "\n",
            "Step No: 114000\n",
            "Episode No: 183\n",
            "Avg reward over the last 100 episodes: -64.01909125512572\n",
            "Highest avg reward logged so far: -63.80607589936644\n",
            "\n",
            "Step No: 115000\n",
            "Episode No: 185\n",
            "Avg reward over the last 100 episodes: -64.17527573993891\n",
            "Highest avg reward logged so far: -63.80607589936644\n",
            "\n",
            "Step No: 116000\n",
            "Episode No: 186\n",
            "Avg reward over the last 100 episodes: -61.0939390681063\n",
            "Highest avg reward logged so far: -61.0939390681063\n",
            "\n",
            "Step No: 117000\n",
            "Episode No: 187\n",
            "Avg reward over the last 100 episodes: -63.74779824503825\n",
            "Highest avg reward logged so far: -61.0939390681063\n",
            "\n",
            "Step No: 118000\n",
            "Episode No: 189\n",
            "Avg reward over the last 100 episodes: -64.80194599535109\n",
            "Highest avg reward logged so far: -61.0939390681063\n",
            "\n",
            "Step No: 119000\n",
            "Episode No: 190\n",
            "Avg reward over the last 100 episodes: -63.55204911751409\n",
            "Highest avg reward logged so far: -61.0939390681063\n",
            "\n",
            "Step No: 120000\n",
            "Episode No: 191\n",
            "Avg reward over the last 100 episodes: -60.43352988070994\n",
            "Highest avg reward logged so far: -60.43352988070994\n",
            "\n",
            "Step No: 121000\n",
            "Episode No: 194\n",
            "Avg reward over the last 100 episodes: -59.18965484329314\n",
            "Highest avg reward logged so far: -59.18965484329314\n",
            "\n",
            "Step No: 122000\n",
            "Episode No: 197\n",
            "Avg reward over the last 100 episodes: -57.22699205892331\n",
            "Highest avg reward logged so far: -57.22699205892331\n",
            "\n",
            "Step No: 123000\n",
            "Episode No: 199\n",
            "Avg reward over the last 100 episodes: -53.28215796650985\n",
            "Highest avg reward logged so far: -53.28215796650985\n",
            "\n",
            "Step No: 124000\n",
            "Episode No: 200\n",
            "Avg reward over the last 100 episodes: -52.73891624700869\n",
            "Highest avg reward logged so far: -52.73891624700869\n",
            "\n",
            "Step No: 125000\n",
            "Episode No: 201\n",
            "Avg reward over the last 100 episodes: -52.406069665232565\n",
            "Highest avg reward logged so far: -52.406069665232565\n",
            "\n",
            "Step No: 126000\n",
            "Episode No: 202\n",
            "Avg reward over the last 100 episodes: -51.387378637250755\n",
            "Highest avg reward logged so far: -51.387378637250755\n",
            "\n",
            "Step No: 127000\n",
            "Episode No: 203\n",
            "Avg reward over the last 100 episodes: -51.1596251491445\n",
            "Highest avg reward logged so far: -51.1596251491445\n",
            "\n",
            "Step No: 128000\n",
            "Episode No: 204\n",
            "Avg reward over the last 100 episodes: -50.345640405167835\n",
            "Highest avg reward logged so far: -50.345640405167835\n",
            "\n",
            "Step No: 129000\n",
            "Episode No: 207\n",
            "Avg reward over the last 100 episodes: -49.22507288232184\n",
            "Highest avg reward logged so far: -49.22507288232184\n",
            "\n",
            "Step No: 130000\n",
            "Episode No: 210\n",
            "Avg reward over the last 100 episodes: -49.05133896461256\n",
            "Highest avg reward logged so far: -49.05133896461256\n",
            "\n",
            "Step No: 131000\n",
            "Episode No: 211\n",
            "Avg reward over the last 100 episodes: -48.58930836480954\n",
            "Highest avg reward logged so far: -48.58930836480954\n",
            "\n",
            "Step No: 132000\n",
            "Episode No: 212\n",
            "Avg reward over the last 100 episodes: -47.977416636387964\n",
            "Highest avg reward logged so far: -47.977416636387964\n",
            "\n",
            "Step No: 133000\n",
            "Episode No: 213\n",
            "Avg reward over the last 100 episodes: -47.57571282182239\n",
            "Highest avg reward logged so far: -47.57571282182239\n",
            "\n",
            "Step No: 134000\n",
            "Episode No: 214\n",
            "Avg reward over the last 100 episodes: -47.08603090743248\n",
            "Highest avg reward logged so far: -47.08603090743248\n",
            "\n",
            "Step No: 135000\n",
            "Episode No: 215\n",
            "Avg reward over the last 100 episodes: -46.170191703255135\n",
            "Highest avg reward logged so far: -46.170191703255135\n",
            "\n",
            "Step No: 136000\n",
            "Episode No: 218\n",
            "Avg reward over the last 100 episodes: -46.89794261447111\n",
            "Highest avg reward logged so far: -46.170191703255135\n",
            "\n",
            "Step No: 137000\n",
            "Episode No: 220\n",
            "Avg reward over the last 100 episodes: -46.887659227899775\n",
            "Highest avg reward logged so far: -46.170191703255135\n",
            "\n",
            "Step No: 138000\n",
            "Episode No: 221\n",
            "Avg reward over the last 100 episodes: -46.42946692126162\n",
            "Highest avg reward logged so far: -46.170191703255135\n",
            "\n",
            "Step No: 139000\n",
            "Episode No: 224\n",
            "Avg reward over the last 100 episodes: -44.58881895449872\n",
            "Highest avg reward logged so far: -44.58881895449872\n",
            "\n",
            "Step No: 140000\n",
            "Episode No: 228\n",
            "Avg reward over the last 100 episodes: -43.44222864108678\n",
            "Highest avg reward logged so far: -43.44222864108678\n",
            "\n",
            "Step No: 141000\n",
            "Episode No: 231\n",
            "Avg reward over the last 100 episodes: -43.403049277121184\n",
            "Highest avg reward logged so far: -43.403049277121184\n",
            "\n",
            "Step No: 142000\n",
            "Episode No: 233\n",
            "Avg reward over the last 100 episodes: -41.7069324971434\n",
            "Highest avg reward logged so far: -41.7069324971434\n",
            "\n",
            "Step No: 143000\n",
            "Episode No: 235\n",
            "Avg reward over the last 100 episodes: -42.58094248502566\n",
            "Highest avg reward logged so far: -41.7069324971434\n",
            "\n",
            "Step No: 144000\n",
            "Episode No: 237\n",
            "Avg reward over the last 100 episodes: -40.22272026353186\n",
            "Highest avg reward logged so far: -40.22272026353186\n",
            "\n",
            "Step No: 145000\n",
            "Episode No: 241\n",
            "Avg reward over the last 100 episodes: -41.27430564151934\n",
            "Highest avg reward logged so far: -40.22272026353186\n",
            "\n",
            "Step No: 146000\n",
            "Episode No: 242\n",
            "Avg reward over the last 100 episodes: -39.75146826526088\n",
            "Highest avg reward logged so far: -39.75146826526088\n",
            "\n",
            "Step No: 147000\n",
            "Episode No: 244\n",
            "Avg reward over the last 100 episodes: -37.57269401427601\n",
            "Highest avg reward logged so far: -37.57269401427601\n",
            "\n",
            "Step No: 148000\n",
            "Episode No: 246\n",
            "Avg reward over the last 100 episodes: -37.55032834426354\n",
            "Highest avg reward logged so far: -37.55032834426354\n",
            "\n",
            "Step No: 149000\n",
            "Episode No: 250\n",
            "Avg reward over the last 100 episodes: -30.67993117168037\n",
            "Highest avg reward logged so far: -30.67993117168037\n",
            "\n",
            "Step No: 150000\n",
            "Episode No: 253\n",
            "Avg reward over the last 100 episodes: -31.91158144989877\n",
            "Highest avg reward logged so far: -30.67993117168037\n",
            "\n",
            "Step No: 151000\n",
            "Episode No: 256\n",
            "Avg reward over the last 100 episodes: -27.28733577155522\n",
            "Highest avg reward logged so far: -27.28733577155522\n",
            "\n",
            "Step No: 152000\n",
            "Episode No: 260\n",
            "Avg reward over the last 100 episodes: -22.24622432478069\n",
            "Highest avg reward logged so far: -22.24622432478069\n",
            "\n",
            "Step No: 153000\n",
            "Episode No: 261\n",
            "Avg reward over the last 100 episodes: -22.969612457719073\n",
            "Highest avg reward logged so far: -22.24622432478069\n",
            "\n",
            "Step No: 154000\n",
            "Episode No: 264\n",
            "Avg reward over the last 100 episodes: -20.005760102554163\n",
            "Highest avg reward logged so far: -20.005760102554163\n",
            "\n",
            "Step No: 155000\n",
            "Episode No: 267\n",
            "Avg reward over the last 100 episodes: -16.065741348942453\n",
            "Highest avg reward logged so far: -16.065741348942453\n",
            "\n",
            "Step No: 156000\n",
            "Episode No: 268\n",
            "Avg reward over the last 100 episodes: -13.911662525465234\n",
            "Highest avg reward logged so far: -13.911662525465234\n",
            "\n",
            "Step No: 157000\n",
            "Episode No: 271\n",
            "Avg reward over the last 100 episodes: -10.166302730842265\n",
            "Highest avg reward logged so far: -10.166302730842265\n",
            "\n",
            "Step No: 158000\n",
            "Episode No: 272\n",
            "Avg reward over the last 100 episodes: -8.285067401826375\n",
            "Highest avg reward logged so far: -8.285067401826375\n",
            "\n",
            "Step No: 159000\n",
            "Episode No: 274\n",
            "Avg reward over the last 100 episodes: -6.051339275805141\n",
            "Highest avg reward logged so far: -6.051339275805141\n",
            "\n",
            "Step No: 160000\n",
            "Episode No: 275\n",
            "Avg reward over the last 100 episodes: -3.050779930657635\n",
            "Highest avg reward logged so far: -3.050779930657635\n",
            "\n",
            "Step No: 161000\n",
            "Episode No: 276\n",
            "Avg reward over the last 100 episodes: -1.4218477441482156\n",
            "Highest avg reward logged so far: -1.4218477441482156\n",
            "\n",
            "Step No: 162000\n",
            "Episode No: 278\n",
            "Avg reward over the last 100 episodes: 2.909249629912996\n",
            "Highest avg reward logged so far: 2.909249629912996\n",
            "\n",
            "Step No: 163000\n",
            "Episode No: 279\n",
            "Avg reward over the last 100 episodes: 3.0426042000225646\n",
            "Highest avg reward logged so far: 3.0426042000225646\n",
            "\n",
            "Step No: 164000\n",
            "Episode No: 282\n",
            "Avg reward over the last 100 episodes: 9.028728956596257\n",
            "Highest avg reward logged so far: 9.028728956596257\n",
            "\n",
            "Step No: 165000\n",
            "Episode No: 284\n",
            "Avg reward over the last 100 episodes: 11.50324028737132\n",
            "Highest avg reward logged so far: 11.50324028737132\n",
            "\n",
            "Step No: 166000\n",
            "Episode No: 287\n",
            "Avg reward over the last 100 episodes: 7.326886055715348\n",
            "Highest avg reward logged so far: 11.50324028737132\n",
            "\n",
            "Step No: 167000\n",
            "Episode No: 289\n",
            "Avg reward over the last 100 episodes: 12.945826373151402\n",
            "Highest avg reward logged so far: 12.945826373151402\n",
            "\n",
            "Step No: 168000\n",
            "Episode No: 292\n",
            "Avg reward over the last 100 episodes: 13.891462522151357\n",
            "Highest avg reward logged so far: 13.891462522151357\n",
            "\n",
            "Step No: 169000\n",
            "Episode No: 293\n",
            "Avg reward over the last 100 episodes: 16.828301553969567\n",
            "Highest avg reward logged so far: 16.828301553969567\n",
            "\n",
            "Step No: 170000\n",
            "Episode No: 295\n",
            "Avg reward over the last 100 episodes: 19.411162187467845\n",
            "Highest avg reward logged so far: 19.411162187467845\n",
            "\n",
            "Step No: 171000\n",
            "Episode No: 297\n",
            "Avg reward over the last 100 episodes: 18.89391068629506\n",
            "Highest avg reward logged so far: 19.411162187467845\n",
            "\n",
            "Step No: 172000\n",
            "Episode No: 298\n",
            "Avg reward over the last 100 episodes: 16.990555970826165\n",
            "Highest avg reward logged so far: 19.411162187467845\n",
            "\n",
            "Step No: 173000\n",
            "Episode No: 302\n",
            "Avg reward over the last 100 episodes: 24.830052777686724\n",
            "Highest avg reward logged so far: 24.830052777686724\n",
            "\n",
            "Step No: 174000\n",
            "Episode No: 304\n",
            "Avg reward over the last 100 episodes: 24.46991347046156\n",
            "Highest avg reward logged so far: 24.830052777686724\n",
            "\n",
            "Step No: 175000\n",
            "Episode No: 306\n",
            "Avg reward over the last 100 episodes: 25.256599012912115\n",
            "Highest avg reward logged so far: 25.256599012912115\n",
            "\n",
            "Step No: 176000\n",
            "Episode No: 309\n",
            "Avg reward over the last 100 episodes: 32.303343379151244\n",
            "Highest avg reward logged so far: 32.303343379151244\n",
            "\n",
            "Step No: 177000\n",
            "Episode No: 310\n",
            "Avg reward over the last 100 episodes: 32.697466401339874\n",
            "Highest avg reward logged so far: 32.697466401339874\n",
            "\n",
            "Step No: 178000\n",
            "Episode No: 312\n",
            "Avg reward over the last 100 episodes: 35.463846670047985\n",
            "Highest avg reward logged so far: 35.463846670047985\n",
            "\n",
            "Step No: 179000\n",
            "Episode No: 315\n",
            "Avg reward over the last 100 episodes: 39.105121135696706\n",
            "Highest avg reward logged so far: 39.105121135696706\n",
            "\n",
            "Step No: 180000\n",
            "Episode No: 316\n",
            "Avg reward over the last 100 episodes: 39.426765384177266\n",
            "Highest avg reward logged so far: 39.426765384177266\n",
            "\n",
            "Step No: 181000\n",
            "Episode No: 318\n",
            "Avg reward over the last 100 episodes: 42.44268324880343\n",
            "Highest avg reward logged so far: 42.44268324880343\n",
            "\n",
            "Step No: 182000\n",
            "Episode No: 319\n",
            "Avg reward over the last 100 episodes: 45.05566509330187\n",
            "Highest avg reward logged so far: 45.05566509330187\n",
            "\n",
            "Step No: 183000\n",
            "Episode No: 320\n",
            "Avg reward over the last 100 episodes: 45.7376177770576\n",
            "Highest avg reward logged so far: 45.7376177770576\n",
            "\n",
            "Step No: 184000\n",
            "Episode No: 322\n",
            "Avg reward over the last 100 episodes: 45.92712066149898\n",
            "Highest avg reward logged so far: 45.92712066149898\n",
            "\n",
            "Step No: 185000\n",
            "Episode No: 324\n",
            "Avg reward over the last 100 episodes: 47.25996318968704\n",
            "Highest avg reward logged so far: 47.25996318968704\n",
            "\n",
            "Step No: 186000\n",
            "Episode No: 325\n",
            "Avg reward over the last 100 episodes: 50.002468130494805\n",
            "Highest avg reward logged so far: 50.002468130494805\n",
            "Agent parameters saved successfully to '/content/Q net params Checkpoint 1.pth'\n",
            "\n",
            "Step No: 187000\n",
            "Episode No: 327\n",
            "Avg reward over the last 100 episodes: 50.12523658043312\n",
            "Highest avg reward logged so far: 50.12523658043312\n",
            "\n",
            "Step No: 188000\n",
            "Episode No: 329\n",
            "Avg reward over the last 100 episodes: 55.80170141065233\n",
            "Highest avg reward logged so far: 55.80170141065233\n",
            "\n",
            "Step No: 189000\n",
            "Episode No: 330\n",
            "Avg reward over the last 100 episodes: 56.001305874098186\n",
            "Highest avg reward logged so far: 56.001305874098186\n",
            "\n",
            "Step No: 190000\n",
            "Episode No: 331\n",
            "Avg reward over the last 100 episodes: 55.91277508514159\n",
            "Highest avg reward logged so far: 56.001305874098186\n",
            "\n",
            "Step No: 191000\n",
            "Episode No: 335\n",
            "Avg reward over the last 100 episodes: 65.72553279352255\n",
            "Highest avg reward logged so far: 65.72553279352255\n",
            "\n",
            "Step No: 192000\n",
            "Episode No: 339\n",
            "Avg reward over the last 100 episodes: 69.11222607231687\n",
            "Highest avg reward logged so far: 69.11222607231687\n",
            "\n",
            "Step No: 193000\n",
            "Episode No: 340\n",
            "Avg reward over the last 100 episodes: 70.75955145310813\n",
            "Highest avg reward logged so far: 70.75955145310813\n",
            "\n",
            "Step No: 194000\n",
            "Episode No: 343\n",
            "Avg reward over the last 100 episodes: 75.30682551285024\n",
            "Highest avg reward logged so far: 75.30682551285024\n",
            "\n",
            "Step No: 195000\n",
            "Episode No: 344\n",
            "Avg reward over the last 100 episodes: 77.75530586018044\n",
            "Highest avg reward logged so far: 77.75530586018044\n",
            "\n",
            "Step No: 196000\n",
            "Episode No: 346\n",
            "Avg reward over the last 100 episodes: 82.73802619900165\n",
            "Highest avg reward logged so far: 82.73802619900165\n",
            "\n",
            "Step No: 197000\n",
            "Episode No: 347\n",
            "Avg reward over the last 100 episodes: 81.2706742880986\n",
            "Highest avg reward logged so far: 82.73802619900165\n",
            "\n",
            "Step No: 198000\n",
            "Episode No: 349\n",
            "Avg reward over the last 100 episodes: 81.4330700651683\n",
            "Highest avg reward logged so far: 82.73802619900165\n",
            "\n",
            "Step No: 199000\n",
            "Episode No: 350\n",
            "Avg reward over the last 100 episodes: 80.78190872535268\n",
            "Highest avg reward logged so far: 82.73802619900165\n",
            "\n",
            "Step No: 200000\n",
            "Episode No: 351\n",
            "Avg reward over the last 100 episodes: 83.32869685485575\n",
            "Highest avg reward logged so far: 83.32869685485575\n",
            "\n",
            "Step No: 201000\n",
            "Episode No: 353\n",
            "Avg reward over the last 100 episodes: 87.04599579193862\n",
            "Highest avg reward logged so far: 87.04599579193862\n",
            "\n",
            "Step No: 202000\n",
            "Episode No: 354\n",
            "Avg reward over the last 100 episodes: 86.93252191277165\n",
            "Highest avg reward logged so far: 87.04599579193862\n",
            "\n",
            "Step No: 203000\n",
            "Episode No: 355\n",
            "Avg reward over the last 100 episodes: 87.63448210071915\n",
            "Highest avg reward logged so far: 87.63448210071915\n",
            "\n",
            "Step No: 204000\n",
            "Episode No: 356\n",
            "Avg reward over the last 100 episodes: 87.00556209814556\n",
            "Highest avg reward logged so far: 87.63448210071915\n",
            "\n",
            "Step No: 205000\n",
            "Episode No: 358\n",
            "Avg reward over the last 100 episodes: 88.25132999295529\n",
            "Highest avg reward logged so far: 88.25132999295529\n",
            "\n",
            "Step No: 206000\n",
            "Episode No: 360\n",
            "Avg reward over the last 100 episodes: 90.14507753111171\n",
            "Highest avg reward logged so far: 90.14507753111171\n",
            "\n",
            "Step No: 207000\n",
            "Episode No: 363\n",
            "Avg reward over the last 100 episodes: 94.03251952547863\n",
            "Highest avg reward logged so far: 94.03251952547863\n",
            "\n",
            "Step No: 208000\n",
            "Episode No: 364\n",
            "Avg reward over the last 100 episodes: 95.49432356640358\n",
            "Highest avg reward logged so far: 95.49432356640358\n",
            "\n",
            "Step No: 209000\n",
            "Episode No: 366\n",
            "Avg reward over the last 100 episodes: 98.65914474013232\n",
            "Highest avg reward logged so far: 98.65914474013232\n",
            "\n",
            "Step No: 210000\n",
            "Episode No: 367\n",
            "Avg reward over the last 100 episodes: 97.40717859698529\n",
            "Highest avg reward logged so far: 98.65914474013232\n",
            "\n",
            "Step No: 211000\n",
            "Episode No: 368\n",
            "Avg reward over the last 100 episodes: 95.40965884136239\n",
            "Highest avg reward logged so far: 98.65914474013232\n",
            "\n",
            "Step No: 212000\n",
            "Episode No: 371\n",
            "Avg reward over the last 100 episodes: 100.9560853882163\n",
            "Highest avg reward logged so far: 100.9560853882163\n",
            "\n",
            "Step No: 213000\n",
            "Episode No: 372\n",
            "Avg reward over the last 100 episodes: 100.62765018391651\n",
            "Highest avg reward logged so far: 100.9560853882163\n",
            "\n",
            "Step No: 214000\n",
            "Episode No: 373\n",
            "Avg reward over the last 100 episodes: 100.7308264352661\n",
            "Highest avg reward logged so far: 100.9560853882163\n",
            "\n",
            "Step No: 215000\n",
            "Episode No: 374\n",
            "Avg reward over the last 100 episodes: 99.54084271668562\n",
            "Highest avg reward logged so far: 100.9560853882163\n",
            "\n",
            "Step No: 216000\n",
            "Episode No: 376\n",
            "Avg reward over the last 100 episodes: 98.26675576970669\n",
            "Highest avg reward logged so far: 100.9560853882163\n",
            "\n",
            "Step No: 217000\n",
            "Episode No: 378\n",
            "Avg reward over the last 100 episodes: 99.03750320130769\n",
            "Highest avg reward logged so far: 100.9560853882163\n",
            "\n",
            "Step No: 218000\n",
            "Episode No: 380\n",
            "Avg reward over the last 100 episodes: 102.78477554252206\n",
            "Highest avg reward logged so far: 102.78477554252206\n",
            "\n",
            "Step No: 219000\n",
            "Episode No: 381\n",
            "Avg reward over the last 100 episodes: 105.32002312277456\n",
            "Highest avg reward logged so far: 105.32002312277456\n",
            "\n",
            "Step No: 220000\n",
            "Episode No: 382\n",
            "Avg reward over the last 100 episodes: 102.55139161883473\n",
            "Highest avg reward logged so far: 105.32002312277456\n",
            "\n",
            "Step No: 221000\n",
            "Episode No: 383\n",
            "Avg reward over the last 100 episodes: 102.00110735080415\n",
            "Highest avg reward logged so far: 105.32002312277456\n",
            "\n",
            "Step No: 222000\n",
            "Episode No: 386\n",
            "Avg reward over the last 100 episodes: 106.47645678225686\n",
            "Highest avg reward logged so far: 106.47645678225686\n",
            "\n",
            "Step No: 223000\n",
            "Episode No: 389\n",
            "Avg reward over the last 100 episodes: 105.4701037347309\n",
            "Highest avg reward logged so far: 106.47645678225686\n",
            "\n",
            "Step No: 224000\n",
            "Episode No: 391\n",
            "Avg reward over the last 100 episodes: 105.596615631932\n",
            "Highest avg reward logged so far: 106.47645678225686\n",
            "\n",
            "Step No: 225000\n",
            "Episode No: 394\n",
            "Avg reward over the last 100 episodes: 109.7646610983374\n",
            "Highest avg reward logged so far: 109.7646610983374\n",
            "\n",
            "Step No: 226000\n",
            "Episode No: 397\n",
            "Avg reward over the last 100 episodes: 116.71764337262401\n",
            "Highest avg reward logged so far: 116.71764337262401\n",
            "\n",
            "Step No: 227000\n",
            "Episode No: 398\n",
            "Avg reward over the last 100 episodes: 116.55434981087869\n",
            "Highest avg reward logged so far: 116.71764337262401\n",
            "\n",
            "Step No: 228000\n",
            "Episode No: 399\n",
            "Avg reward over the last 100 episodes: 116.61317598484703\n",
            "Highest avg reward logged so far: 116.71764337262401\n",
            "\n",
            "Step No: 229000\n",
            "Episode No: 403\n",
            "Avg reward over the last 100 episodes: 121.55552595398387\n",
            "Highest avg reward logged so far: 121.55552595398387\n",
            "\n",
            "Step No: 230000\n",
            "Episode No: 405\n",
            "Avg reward over the last 100 episodes: 128.63704086485433\n",
            "Highest avg reward logged so far: 128.63704086485433\n",
            "\n",
            "Step No: 231000\n",
            "Episode No: 406\n",
            "Avg reward over the last 100 episodes: 128.69561948202136\n",
            "Highest avg reward logged so far: 128.69561948202136\n",
            "\n",
            "Step No: 232000\n",
            "Episode No: 409\n",
            "Avg reward over the last 100 episodes: 127.34721720650305\n",
            "Highest avg reward logged so far: 128.69561948202136\n",
            "\n",
            "Step No: 233000\n",
            "Episode No: 411\n",
            "Avg reward over the last 100 episodes: 132.35740679620548\n",
            "Highest avg reward logged so far: 132.35740679620548\n",
            "\n",
            "Step No: 234000\n",
            "Episode No: 414\n",
            "Avg reward over the last 100 episodes: 133.97406130212693\n",
            "Highest avg reward logged so far: 133.97406130212693\n",
            "\n",
            "Step No: 235000\n",
            "Episode No: 416\n",
            "Avg reward over the last 100 episodes: 136.4666412822367\n",
            "Highest avg reward logged so far: 136.4666412822367\n",
            "\n",
            "Step No: 236000\n",
            "Episode No: 421\n",
            "Avg reward over the last 100 episodes: 144.22030136185433\n",
            "Highest avg reward logged so far: 144.22030136185433\n",
            "\n",
            "Step No: 237000\n",
            "Episode No: 424\n",
            "Avg reward over the last 100 episodes: 147.3475530977508\n",
            "Highest avg reward logged so far: 147.3475530977508\n",
            "\n",
            "Step No: 238000\n",
            "Episode No: 425\n",
            "Avg reward over the last 100 episodes: 146.92805033147678\n",
            "Highest avg reward logged so far: 147.3475530977508\n",
            "\n",
            "Step No: 239000\n",
            "Episode No: 429\n",
            "Avg reward over the last 100 episodes: 149.9302610795626\n",
            "Highest avg reward logged so far: 149.9302610795626\n",
            "\n",
            "Step No: 240000\n",
            "Episode No: 432\n",
            "Avg reward over the last 100 episodes: 153.66697961947173\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "Agent parameters saved successfully to '/content/Q net params Checkpoint 2.pth'\n",
            "\n",
            "Step No: 241000\n",
            "Episode No: 434\n",
            "Avg reward over the last 100 episodes: 151.03848453492247\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 242000\n",
            "Episode No: 437\n",
            "Avg reward over the last 100 episodes: 150.77905571378056\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 243000\n",
            "Episode No: 440\n",
            "Avg reward over the last 100 episodes: 152.17930986618407\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 244000\n",
            "Episode No: 444\n",
            "Avg reward over the last 100 episodes: 146.6452602663799\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 245000\n",
            "Episode No: 448\n",
            "Avg reward over the last 100 episodes: 147.59247016764238\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 246000\n",
            "Episode No: 451\n",
            "Avg reward over the last 100 episodes: 146.54121112423778\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 247000\n",
            "Episode No: 454\n",
            "Avg reward over the last 100 episodes: 146.83704626630492\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 248000\n",
            "Episode No: 456\n",
            "Avg reward over the last 100 episodes: 150.56925433138855\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 249000\n",
            "Episode No: 457\n",
            "Avg reward over the last 100 episodes: 152.36313461749708\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 250000\n",
            "Episode No: 461\n",
            "Avg reward over the last 100 episodes: 150.51599195339412\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 251000\n",
            "Episode No: 462\n",
            "Avg reward over the last 100 episodes: 150.26141145131618\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 252000\n",
            "Episode No: 463\n",
            "Avg reward over the last 100 episodes: 150.79131622168558\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 253000\n",
            "Episode No: 466\n",
            "Avg reward over the last 100 episodes: 149.84752551160452\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 254000\n",
            "Episode No: 467\n",
            "Avg reward over the last 100 episodes: 151.207458145745\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 255000\n",
            "Episode No: 469\n",
            "Avg reward over the last 100 episodes: 152.06719806047798\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 256000\n",
            "Episode No: 471\n",
            "Avg reward over the last 100 episodes: 148.49629879256005\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 257000\n",
            "Episode No: 473\n",
            "Avg reward over the last 100 episodes: 150.66741294152575\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 258000\n",
            "Episode No: 474\n",
            "Avg reward over the last 100 episodes: 151.10761928164942\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 259000\n",
            "Episode No: 478\n",
            "Avg reward over the last 100 episodes: 148.1936230587044\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 260000\n",
            "Episode No: 480\n",
            "Avg reward over the last 100 episodes: 148.37846895486717\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 261000\n",
            "Episode No: 482\n",
            "Avg reward over the last 100 episodes: 147.4898754745267\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 262000\n",
            "Episode No: 485\n",
            "Avg reward over the last 100 episodes: 150.13415880254763\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 263000\n",
            "Episode No: 486\n",
            "Avg reward over the last 100 episodes: 149.2249002723602\n",
            "Highest avg reward logged so far: 153.66697961947173\n",
            "\n",
            "Step No: 264000\n",
            "Episode No: 490\n",
            "Avg reward over the last 100 episodes: 157.884878784032\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 265000\n",
            "Episode No: 492\n",
            "Avg reward over the last 100 episodes: 155.8621965608526\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 266000\n",
            "Episode No: 494\n",
            "Avg reward over the last 100 episodes: 153.64845368059576\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 267000\n",
            "Episode No: 497\n",
            "Avg reward over the last 100 episodes: 153.56917785033409\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 268000\n",
            "Episode No: 502\n",
            "Avg reward over the last 100 episodes: 147.64601718212452\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 269000\n",
            "Episode No: 503\n",
            "Avg reward over the last 100 episodes: 146.82768079986627\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 270000\n",
            "Episode No: 504\n",
            "Avg reward over the last 100 episodes: 145.4576212278373\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 271000\n",
            "Episode No: 506\n",
            "Avg reward over the last 100 episodes: 143.8125576443459\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 272000\n",
            "Episode No: 507\n",
            "Avg reward over the last 100 episodes: 144.37981781335816\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 273000\n",
            "Episode No: 511\n",
            "Avg reward over the last 100 episodes: 144.604375001962\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 274000\n",
            "Episode No: 512\n",
            "Avg reward over the last 100 episodes: 145.91688165585293\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 275000\n",
            "Episode No: 516\n",
            "Avg reward over the last 100 episodes: 146.7774410096984\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 276000\n",
            "Episode No: 518\n",
            "Avg reward over the last 100 episodes: 149.304602190188\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 277000\n",
            "Episode No: 522\n",
            "Avg reward over the last 100 episodes: 150.5674570480416\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 278000\n",
            "Episode No: 526\n",
            "Avg reward over the last 100 episodes: 149.1910196252407\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 279000\n",
            "Episode No: 530\n",
            "Avg reward over the last 100 episodes: 150.7846699456114\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 280000\n",
            "Episode No: 531\n",
            "Avg reward over the last 100 episodes: 152.94317934758254\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 281000\n",
            "Episode No: 533\n",
            "Avg reward over the last 100 episodes: 153.2061309877169\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 282000\n",
            "Episode No: 534\n",
            "Avg reward over the last 100 episodes: 151.9949904852677\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 283000\n",
            "Episode No: 537\n",
            "Avg reward over the last 100 episodes: 149.05688831978912\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 284000\n",
            "Episode No: 539\n",
            "Avg reward over the last 100 episodes: 152.17128133828535\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 285000\n",
            "Episode No: 542\n",
            "Avg reward over the last 100 episodes: 155.2376750779462\n",
            "Highest avg reward logged so far: 157.884878784032\n",
            "\n",
            "Step No: 286000\n",
            "Episode No: 544\n",
            "Avg reward over the last 100 episodes: 160.9674676699458\n",
            "Highest avg reward logged so far: 160.9674676699458\n",
            "\n",
            "Step No: 287000\n",
            "Episode No: 545\n",
            "Avg reward over the last 100 episodes: 162.33104439334068\n",
            "Highest avg reward logged so far: 162.33104439334068\n",
            "\n",
            "Step No: 288000\n",
            "Episode No: 547\n",
            "Avg reward over the last 100 episodes: 164.24095924765655\n",
            "Highest avg reward logged so far: 164.24095924765655\n",
            "\n",
            "Step No: 289000\n",
            "Episode No: 550\n",
            "Avg reward over the last 100 episodes: 162.2413738912971\n",
            "Highest avg reward logged so far: 164.24095924765655\n",
            "\n",
            "Step No: 290000\n",
            "Episode No: 553\n",
            "Avg reward over the last 100 episodes: 162.70886457929478\n",
            "Highest avg reward logged so far: 164.24095924765655\n",
            "\n",
            "Step No: 291000\n",
            "Episode No: 555\n",
            "Avg reward over the last 100 episodes: 165.164973519454\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 292000\n",
            "Episode No: 558\n",
            "Avg reward over the last 100 episodes: 164.7038170860799\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 293000\n",
            "Episode No: 562\n",
            "Avg reward over the last 100 episodes: 162.95072922300938\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 294000\n",
            "Episode No: 566\n",
            "Avg reward over the last 100 episodes: 161.22633054339957\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 295000\n",
            "Episode No: 568\n",
            "Avg reward over the last 100 episodes: 163.0150836653551\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 296000\n",
            "Episode No: 570\n",
            "Avg reward over the last 100 episodes: 163.48790259823198\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 297000\n",
            "Episode No: 576\n",
            "Avg reward over the last 100 episodes: 158.12733054719428\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 298000\n",
            "Episode No: 579\n",
            "Avg reward over the last 100 episodes: 160.86638296615266\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 299000\n",
            "Episode No: 582\n",
            "Avg reward over the last 100 episodes: 164.87583079578584\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 300000\n",
            "Episode No: 586\n",
            "Avg reward over the last 100 episodes: 164.79667099535646\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 301000\n",
            "Episode No: 587\n",
            "Avg reward over the last 100 episodes: 163.97932890222538\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 302000\n",
            "Episode No: 589\n",
            "Avg reward over the last 100 episodes: 159.5186126012153\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 303000\n",
            "Episode No: 590\n",
            "Avg reward over the last 100 episodes: 158.40570982993364\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 304000\n",
            "Episode No: 594\n",
            "Avg reward over the last 100 episodes: 158.73908398878513\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n",
            "Step No: 305000\n",
            "Episode No: 597\n",
            "Avg reward over the last 100 episodes: 157.27534507414833\n",
            "Highest avg reward logged so far: 165.164973519454\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run below cell if you force stopped the training (above) cell's execution\n",
        "After force stopping the previous cell's execution if required (reward logs had no improvement over a long period), save the latest training state of the q_net parameters as a checkpoint.\n",
        "\n",
        "This was required because I set the last checkpoint's reward value too high as I was not sure what the max reward was, ik it was written 200 in documentation but my model kept wasting fuel even after landing properly, so I wanted to get a smarter model with a higher reward. Through this I found 239 as the highest possible avg reward, so later I set the checkpoint 4 threshold as 230."
      ],
      "metadata": {
        "id": "MxzxCXnQOldD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_net_params_filepath = Q_net_params_filepath[:-4] + ' Checkpoint 4' + '.pth'\n",
        "torch.save(q_net.state_dict(), q_net_params_filepath)\n",
        "print(f\"Agent parameters saved successfully to '{q_net_params_filepath}'\")"
      ],
      "metadata": {
        "id": "lRrX9k1jNyv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See the gameplay of the trained agent at different checkpoints\n",
        "\n",
        "Rather than doing it in the training loop, like I did in the CartPole environment, due to which the training slows down and the download of the video is only done at the end by colab when the execution of all the cells is finished, it is better to do it by saving the model at different checkpoints and checking out the different checkpoints later like done below."
      ],
      "metadata": {
        "id": "6IKRNG7zu7W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames = []\n",
        "num_episodes = 4\n",
        "\n",
        "# The checkpoint number in the training of the model that we want to see the gameplay of\n",
        "checkpoint_number = 4\n",
        "\n",
        "saved_weights_filename = f'Q net params Checkpoint {checkpoint_number}.pth'\n",
        "saved_weights_filepath = os.path.join(nb_dir, saved_weights_filename)\n",
        "saved_weights = torch.load(saved_weights_filepath)\n",
        "q_net.load_state_dict(saved_weights)\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()\n",
        "  while True:\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    frames.append(_label_with_episode_number(frame, episode_num=episode))\n",
        "    action = q_net.act(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "file_name = f'Trained DQN Agent Checkpoint {checkpoint_number}.mp4'\n",
        "download_gameplay_video(frames_list=frames, mp4_file_name=file_name, frames_per_second=30, playback_speed=1)"
      ],
      "metadata": {
        "id": "fTo9LH9uu6gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the results using plotly"
      ],
      "metadata": {
        "id": "Hu2orNQH1yPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly --upgrade --quiet\n",
        "# Need latest version of plotly and need to pass renderers like this, for plotly to work in colab.\n",
        "# Else you need to manually code a function, as plotly requires custom initialization."
      ],
      "metadata": {
        "id": "Qf62mn1N1dEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.offline as pyo\n",
        "pyo.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "zPk5Shh61cXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(episode_reward_dict):\n",
        "  x_axis_vals = list(episode_reward_dict.keys())\n",
        "  y_axis_vals = list(episode_reward_dict.values())\n",
        "  y_label = 'Episode No.'\n",
        "  x_label = 'Avg reward over last 100 episodes'\n",
        "\n",
        "  y_ticks = [-300, -200, -100, 0, 100, 200]\n",
        "  x_ticks = [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "\n",
        "  scatter = go.Scatter(\n",
        "      x=x_axis_vals,\n",
        "      y=y_axis_vals,\n",
        "      mode='lines', # lines+markers, markers\n",
        "      name=f\"{y_label} vs {x_label}\"\n",
        "  )\n",
        "\n",
        "  layout = go.Layout(\n",
        "      title={'text': f\"{y_label} vs {x_label}\", 'x': 0.5, 'y': 0.95},\n",
        "      xaxis=dict( title=f\"{x_label}\", titlefont_size=14, tickfont_size=12, range=[0, 500], tickvals= x_ticks),\n",
        "      yaxis=dict( title=f\"{y_label}\", titlefont_size=14, tickfont_size=12, range=[-300, 200], tickvals= y_ticks ),\n",
        "      autosize=False,\n",
        "      width=600, height=400, margin=dict(l=100, r=50, b=100, t=100, pad=5),\n",
        "      paper_bgcolor='LightSteelBlue',\n",
        "      xaxis_showgrid=True,\n",
        "      yaxis_showgrid=True,\n",
        "  )\n",
        "\n",
        "  fig = go.Figure(data=[scatter], layout=layout)\n",
        "\n",
        "  fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "GIwJWFtjzyIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(episode_reward_dict=episode_avg_reward_dict)"
      ],
      "metadata": {
        "id": "vWYqcONPETdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load saved weights for the Q and target Q networks"
      ],
      "metadata": {
        "id": "RSjggZrbgKNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_weights(Q_net_params_filepath, target_Q_net_params_filepath):\n",
        "  if os.path.exists(target_Q_net_params_filepath) and os.path.exists(Q_net_params_filepath):\n",
        "    q_net.load_state_dict(torch.load(Q_net_params_filepath))\n",
        "    target_q_net.load_state_dict(torch.load(target_Q_net_params_filepath))\n",
        "    optimizer = torch.optim.Adam(q_net.parameters(), lr=ALPHA)\n",
        "  else:\n",
        "    print('No saved parameters for the neural nets!')"
      ],
      "metadata": {
        "id": "nSe2C-tYgHcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Etf6qRlgUXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c450a0c1f18e42f1a42e85ad966eea98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e53c80b3f60c4408b5b0b3762b03a186",
              "IPY_MODEL_b0e471d558e242c49dc24939a52fb9c8",
              "IPY_MODEL_af3361d8f7f9441ebea67cfcb9bb211f"
            ],
            "layout": "IPY_MODEL_309a6f9539fd4b738a9fb1f649d55158"
          }
        },
        "e53c80b3f60c4408b5b0b3762b03a186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_127ba5b0bad24db5a1c456fde70f9ee0",
            "placeholder": "​",
            "style": "IPY_MODEL_0e5b9fb30c794269820e8a518bca0dad",
            "value": "Training...:  30%"
          }
        },
        "b0e471d558e242c49dc24939a52fb9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_799d065ddb814e75acb79d542aeb78c5",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f2d409c1f0f40d6967633e890981334",
            "value": 599
          }
        },
        "af3361d8f7f9441ebea67cfcb9bb211f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa31c8ba96044c2a3e788e956b2ff5e",
            "placeholder": "​",
            "style": "IPY_MODEL_28f656d85a884cbfabadb9dc1b0c03d3",
            "value": " 598/2000 [18:46&lt;42:35,  1.82s/it]"
          }
        },
        "309a6f9539fd4b738a9fb1f649d55158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "127ba5b0bad24db5a1c456fde70f9ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e5b9fb30c794269820e8a518bca0dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "799d065ddb814e75acb79d542aeb78c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f2d409c1f0f40d6967633e890981334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aa31c8ba96044c2a3e788e956b2ff5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28f656d85a884cbfabadb9dc1b0c03d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}